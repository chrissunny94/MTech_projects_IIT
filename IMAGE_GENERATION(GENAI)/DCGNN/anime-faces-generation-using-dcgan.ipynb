{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# <p><center style=\"font-family:newtimeroman;font-size:180%;\"> Generating Anime Faces with DCGAN: A Dataset of High-Quality Anime </center></p>\n",
    "### Table of contents:\n",
    "\n",
    "* [Introduction](#1)\n",
    "* [Import Libraries](#2)\n",
    "* [Import DataSet](#3)\n",
    "* [Preprocessing](#4)\n",
    "* [Create Generator](#5)\n",
    "* [Create Discriminator](#6)\n",
    "* [Create Deep Convolutional GAN](#7)\n",
    "* [Train The Model](#8)\n",
    "* [Evaluation Of Model Results](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T19:17:39.967019Z",
     "iopub.status.busy": "2023-08-14T19:17:39.966306Z"
    }
   },
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "  <h1>Generating Anime Faces with DCGAN: A Dataset of High-Quality Anime Girls</h1>\n",
    "  <p> In this project, we aim to generate high-quality anime faces using the Deep Convolutional Generative Adversarial Networks (DCGAN) algorithm. The dataset used for this project consists of 63,632 anime faces, carefully curated to ensure quality and appeal.\n",
    "The motivation behind this project is to fulfill the simple dream of generating perfect waifus, cute female anime faces that capture the essence of the anime art style. \n",
    "The DCGAN algorithm offers an attractive approach to generating realistic and visually appealing anime faces. By training the generator and discriminator networks in an adversarial learning process, we can learn a hierarchy of representations, starting from object parts and progressing to scenes. This allows us to create compelling and diverse anime face images.\n",
    "To showcase the capabilities of the project, we provide examples of both real and generated anime face images. By comparing the \"real vs. fake\" images, viewers can appreciate the quality and realism achieved through the DCGAN algorithm.\n",
    "By combining the power of DCGAN and a meticulously curated anime face dataset, we aim to contribute to the world of anime art and provide a valuable resource for researchers, artists, and fans alike. Join us on this exciting journey to generate captivating anime faces and bring your favorite characters to life!</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mtensorflow.python.framework.errors_impl.NotFoundError: Could not find directory animefacedataset. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !pip install -r requirement.txt\n",
    "# !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "# import protobuf\n",
    "\n",
    "\n",
    "# print(\"protobuf version:\", protobuf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:24:51.293865Z",
     "iopub.status.busy": "2024-01-17T09:24:51.293000Z",
     "iopub.status.idle": "2024-01-17T09:25:03.076037Z",
     "shell.execute_reply": "2024-01-17T09:25:03.075044Z",
     "shell.execute_reply.started": "2024-01-17T09:24:51.293833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import requirement libraries and tools\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style= \"darkgrid\", color_codes = True)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Conv2DTranspose, Reshape, BatchNormalization, Dropout, Input, ReLU, LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import DataSet** \n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:28:18.308325Z",
     "iopub.status.busy": "2024-01-17T09:28:18.307109Z",
     "iopub.status.idle": "2024-01-17T09:32:28.046197Z",
     "shell.execute_reply": "2024-01-17T09:32:28.045401Z",
     "shell.execute_reply.started": "2024-01-17T09:28:18.308287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading and Preparing Anime Face Images Dataset using Keras Image Data Generator\n",
    "img_width, img_height = 256, 256\n",
    "batchsize = 32\n",
    "\n",
    "train = keras. utils.image_dataset_from_directory(\n",
    "    directory='/media/chris/WINDOWS_PARTITION/GENAI/animefacedataset',\n",
    "    batch_size = batchsize,\n",
    "    image_size = (img_width, img_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing**\n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:32:31.271823Z",
     "iopub.status.busy": "2024-01-17T09:32:31.271020Z",
     "iopub.status.idle": "2024-01-17T09:32:33.324694Z",
     "shell.execute_reply": "2024-01-17T09:32:33.323626Z",
     "shell.execute_reply.started": "2024-01-17T09:32:31.271790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizing a Batch of Anime Face Images\n",
    "\n",
    "data_iterator = train.as_numpy_iterator()\n",
    "batch = data_iterator.next()\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(10,10))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle\n",
    "#!kaggle datasets download -d splcher/animefacedataset\n",
    "#!unzip animefacedataset.zip -d animefacedataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:32:40.364060Z",
     "iopub.status.busy": "2024-01-17T09:32:40.363291Z",
     "iopub.status.idle": "2024-01-17T09:32:54.072407Z",
     "shell.execute_reply": "2024-01-17T09:32:54.071606Z",
     "shell.execute_reply.started": "2024-01-17T09:32:40.364026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generating Augmented Batches of Anime Face Images using ImageDataGenerator\n",
    "DIR = 'animefacedataset' #path\n",
    "\n",
    "# Create an ImageDataGenerator object with data augmentation options for image preprocessing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        DIR,\n",
    "        target_size = (64, 64),\n",
    "        batch_size = batchsize,\n",
    "        class_mode = None)\n",
    "\n",
    "#train_generator[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Deep Convolutional Generative Adversarial Network</h1>\n",
    "  <p>DCGAN (Deep Convolutional Generative Adversarial Network) is an advanced architecture and training methodology for generative adversarial networks (GANs) specifically designed for image synthesis tasks. It combines deep convolutional neural networks with the adversarial learning framework to generate high-quality and realistic images. </p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Generator**\n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "  <h1> The Generator </h1>\n",
    "  <p>In DCGAN, the generator and discriminator networks play crucial roles. The generator is responsible for generating synthetic images that resemble the target data distribution. It takes random noise as input and gradually transforms it into higher-dimensional outputs using convolutional layers, transposed convolutions, and activation functions like ReLU. Batch normalization is often used to stabilize the learning process. </p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:35:11.391789Z",
     "iopub.status.busy": "2024-01-17T09:35:11.391096Z",
     "iopub.status.idle": "2024-01-17T09:35:11.812727Z",
     "shell.execute_reply": "2024-01-17T09:35:11.811774Z",
     "shell.execute_reply.started": "2024-01-17T09:35:11.391755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating the Generator Model \n",
    "\n",
    "KI = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "input_dim = 300\n",
    "\n",
    "def Generator_Model():\n",
    "\n",
    "    Generator = Sequential()\n",
    "\n",
    "    # Random noise\n",
    "    Generator.add(Dense(8 * 8 * 512, input_dim = input_dim))\n",
    "    Generator.add(ReLU())\n",
    "    # Convert 1d to 3d\n",
    "    Generator.add(Reshape((8, 8, 512)))\n",
    "    # Unsample\n",
    "    Generator.add(Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=KI, activation='ReLU'))\n",
    "    Generator.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=KI, activation='ReLU'))\n",
    "    Generator.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=KI, activation='ReLU'))\n",
    "    Generator.add(Conv2D(3, (4, 4), padding='same', activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    return Generator\n",
    "    \n",
    "generator = Generator_Model()\n",
    "generator.summary()\n",
    "# Visualized Layers of generator\n",
    "keras.utils.plot_model(generator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Discriminator**\n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "  <h1> The Discriminator </h1>\n",
    "  <p> The discriminator, on the other hand, aims to distinguish between real and generated images. It utilizes convolutional layers, activation functions, and strided convolutions to downsample the spatial dimensions and capture image features. The discriminator is trained to maximize its ability to correctly classify images as real or fake. </p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:36:23.962224Z",
     "iopub.status.busy": "2024-01-17T09:36:23.961830Z",
     "iopub.status.idle": "2024-01-17T09:36:24.176669Z",
     "shell.execute_reply": "2024-01-17T09:36:24.175602Z",
     "shell.execute_reply.started": "2024-01-17T09:36:23.962196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating the discriminator Model \n",
    "\n",
    "def Discriminator_Model():\n",
    "    input_shape = (64, 64, 3)\n",
    "\n",
    "    # Create a Sequential model\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Conv2D(64,kernel_size=(3, 3), activation='LeakyReLU', input_shape = input_shape))\n",
    "    discriminator.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    discriminator.add(Conv2D(128, kernel_size=(3, 3), activation='LeakyReLU'))\n",
    "    discriminator.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    discriminator.add(Conv2D(256, kernel_size=(3, 3), activation='LeakyReLU'))\n",
    "    discriminator.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(256, activation='LeakyReLU'))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "# Training The CNN\n",
    "discriminator = Discriminator_Model()\n",
    "discriminator.summary()  \n",
    "# Visualized Layers of discriminator\n",
    "keras.utils.plot_model(discriminator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Deep Convolutional GAN**\n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "  <h1> The Training Process </h1>\n",
    "  <p> The training process of DCGAN involves an adversarial interplay between the generator and discriminator. The generator aims to generate increasingly realistic images to deceive the discriminator, while the discriminator strives to improve its discrimination ability. This iterative process continues until the generator produces images that are visually convincing and indistinguishable from real images.\n",
    "During training, the generator and discriminator networks are updated using techniques like stochastic gradient descent (SGD) or Adam optimization. The Binary Cross Entropy loss function is commonly used to compute the difference between predicted probabilities and target labels for both networks. </p>\n",
    "</body>\n",
    "</html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:37:29.480384Z",
     "iopub.status.busy": "2024-01-17T09:37:29.479657Z",
     "iopub.status.idle": "2024-01-17T09:37:29.494640Z",
     "shell.execute_reply": "2024-01-17T09:37:29.493619Z",
     "shell.execute_reply.started": "2024-01-17T09:37:29.480354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DCGAN Model Training Step with Discriminator and Generator\n",
    "\n",
    "class DCGAN(keras.Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim = input_dim):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
    "        self.d_loss_metric = keras.metrics.Mean(name='d_loss')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_loss_metric]\n",
    "    \n",
    "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def train_step(self, real_images):\n",
    "        # get batch size from the data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        # generate random noise\n",
    "        random_noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        \n",
    "        # train the discriminator with real (1) and fake (0) images\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute loss on real images\n",
    "            pred_real = self.discriminator(real_images, training=True)\n",
    "            # generate real image labels\n",
    "            real_labels = tf.ones((batch_size, 1))\n",
    "            # label smoothing\n",
    "            real_labels += 0.05 * tf.random.uniform(tf.shape(real_labels))\n",
    "            d_loss_real = self.loss_fn(real_labels, pred_real)\n",
    "            \n",
    "            # compute loss on fake images\n",
    "            fake_images = self.generator(random_noise)\n",
    "            pred_fake = self.discriminator(fake_images, training=True)\n",
    "            # generate fake labels\n",
    "            fake_labels = tf.zeros((batch_size, 1))\n",
    "            d_loss_fake = self.loss_fn(fake_labels, pred_fake)\n",
    "            \n",
    "            # total discriminator loss\n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            \n",
    "        # compute discriminator gradients\n",
    "        gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        # update the gradients\n",
    "        self.d_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        \n",
    "        # train the generator model\n",
    "        labels = tf.ones((batch_size, 1))\n",
    "        # generator want discriminator to think that fake images are real\n",
    "        with tf.GradientTape() as tape:\n",
    "            # generate fake images from generator\n",
    "            fake_images = self.generator(random_noise, training=True)\n",
    "            # classify images as real or fake\n",
    "            pred_fake = self.discriminator(fake_images, training=True)\n",
    "            # compute loss\n",
    "            g_loss = self.loss_fn(labels, pred_fake)\n",
    "            \n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # update the gradients\n",
    "        self.g_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        # update states for both models\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        \n",
    "        return {'d_loss': self.d_loss_metric.result(), 'g_loss': self.g_loss_metric.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "  <h1> The Monitoring process </h1>\n",
    "  <p> To monitor the training progress, callbacks like the DCGANMonitor can be used. This callback generates images from random noise using the trained generator and visualizes them. Additionally, the generator can be saved at the end of training for future use. </p>\n",
    "</body>\n",
    "</html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:37:44.222350Z",
     "iopub.status.busy": "2024-01-17T09:37:44.221966Z",
     "iopub.status.idle": "2024-01-17T09:37:44.229208Z",
     "shell.execute_reply": "2024-01-17T09:37:44.228086Z",
     "shell.execute_reply.started": "2024-01-17T09:37:44.222319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DCGAN Monitor for Image Generation and Model Saving\n",
    "\n",
    "class DCGANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_imgs=25, latent_dim = input_dim):\n",
    "        self.num_imgs = num_imgs\n",
    "        self.latent_dim = latent_dim\n",
    "        # create random noise for generating images\n",
    "        self.noise = tf.random.normal([25, latent_dim])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        # generate the image from noise\n",
    "        g_img = self.model.generator(self.noise)\n",
    "        # denormalize the image\n",
    "        g_img = (g_img * 255) + 255\n",
    "        g_img.numpy()\n",
    "        \n",
    "    def on_train_end(self, logs = None):\n",
    "        self.model.generator.save('DCGEN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:25:02.772215Z",
     "iopub.status.busy": "2023-10-11T17:25:02.771579Z",
     "iopub.status.idle": "2023-10-11T17:25:02.780734Z",
     "shell.execute_reply": "2023-10-11T17:25:02.779830Z",
     "shell.execute_reply.started": "2023-10-11T17:25:02.772183Z"
    }
   },
   "source": [
    "## **Train The Model** \n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T09:38:23.167025Z",
     "iopub.status.busy": "2024-01-17T09:38:23.166183Z",
     "iopub.status.idle": "2024-01-17T10:28:22.968926Z",
     "shell.execute_reply": "2024-01-17T10:28:22.968072Z",
     "shell.execute_reply.started": "2024-01-17T09:38:23.166991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training DCGAN on Image Dataset for 40 Epochs\n",
    "\n",
    "epochs = 30\n",
    "lr_g =0.0003\n",
    "lr_d = 0.0001\n",
    "beta = 0.5\n",
    "latent_dim = 300\n",
    "\n",
    "dcgan = DCGAN(generator=generator, discriminator=discriminator, latent_dim = latent_dim )\n",
    "dcgan.compile(g_optimizer = Adam (learning_rate= lr_g, beta_1= beta), d_optimizer= Adam (learning_rate = lr_g , beta_1= beta), loss_fn = BinaryCrossentropy())\n",
    "\n",
    "# Fit the model and save the history\n",
    "history = dcgan.fit(train_generator, epochs=epochs, callbacks=[DCGANMonitor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation Of Model Results**\n",
    "<a class=\"btn\" href=\"#home\">Tabel of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T10:37:02.269839Z",
     "iopub.status.busy": "2024-01-17T10:37:02.269145Z",
     "iopub.status.idle": "2024-01-17T10:37:06.033035Z",
     "shell.execute_reply": "2024-01-17T10:37:06.032003Z",
     "shell.execute_reply.started": "2024-01-17T10:37:02.269805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generating 36 Random Images with DCGAN\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(36):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    # Generate random noise for each image\n",
    "    noise = tf.random.normal([1, 300])\n",
    "    mg = dcgan.generator(noise)\n",
    "    # Denormalize\n",
    "    mg = (mg * 255) + 255\n",
    "\n",
    "    mg.numpy()\n",
    "    image = Image.fromarray(np.uint8(mg[0]))\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T10:37:11.422090Z",
     "iopub.status.busy": "2024-01-17T10:37:11.421641Z",
     "iopub.status.idle": "2024-01-17T10:37:11.790603Z",
     "shell.execute_reply": "2024-01-17T10:37:11.789675Z",
     "shell.execute_reply.started": "2024-01-17T10:37:11.422056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create a figure for the losses\n",
    "def create_loss_figure(d_loss_values, g_loss_values):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(d_loss_values, label='Discriminator Loss')\n",
    "    plt.plot(g_loss_values, label='Generator Loss')\n",
    "    plt.title('Generator and Discriminator Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Access the loss values from the history\n",
    "d_loss_values = history.history['d_loss']\n",
    "g_loss_values = history.history['g_loss']\n",
    "\n",
    "# Call the create_loss_figure function with the loss values\n",
    "create_loss_figure(d_loss_values, g_loss_values)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 379764,
     "sourceId": 737475,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "GENAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
